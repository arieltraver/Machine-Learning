{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 6.036_Homework3_Spring2022.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arieltraver/Machine-Learning/blob/main/Copy_of_6_036_Homework3_Spring2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.036 Spring 2022 Homework 3\n",
        "\n",
        "**If you haven't already, please hit :**\n",
        "\n",
        "`File` -> `Save a Copy in Drive`\n",
        "\n",
        "**to copy this notebook to your Google drive, and work on a copy. If you don't do this, your changes won't be saved!**"
      ],
      "metadata": {
        "id": "USxWiWVJmnAk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asA46P9SBahm"
      },
      "outputs": [],
      "source": [
        "# Run this cell to download the test functions for HW 3\n",
        "!rm -f hw03_tests.py\n",
        "!wget --quiet --no-check-certificate https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw03/hw03_tests.py\n",
        "from hw03_tests import *\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(X, y, J, dJ, w0, step_size_fn, max_iter):\n",
        "    w = w0\n",
        "    for t in range(max_iter):\n",
        "        i = np.random.randint(y.size)\n",
        "        data = X[:,i:1+i]\n",
        "        label = y[0:1,i:1+i]\n",
        "        w = w - step_size_fn(t)*dJ(data, label, w)\n",
        "    return w\n",
        "\n",
        "def make_num_grad_fn(f, delta=0.001):\n",
        "    def df(x):\n",
        "        Delta = delta * np.identity(x.shape[0])\n",
        "        g = (f(x + Delta) - f(x - Delta)) / (2*delta)\n",
        "        return g.T\n",
        "    return df\n",
        "\n",
        "TestSGD(sgd, make_num_grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvEtaw4GuYJo",
        "outputId": "3081fde7-6f62-4402-820c-069a50aadba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test case passed!\n",
            "----------------------------\n",
            "Test case passed!\n",
            "----------------------------\n",
            "\n",
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymWFpDbDqsbR"
      },
      "source": [
        "#### 10.2 Finding the Best Parameters (Optional)\n",
        "\n",
        "Let's load the Boston Housing dataset. Our goal is to build a linear regression model (with regularization) to predict the TARGET_VAL (which is the median value of owner-occupied homes) using all other available features in the dataset.\n",
        "\n",
        "For more information about the Boston housing dataset, please visit this [link](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). \n",
        "\n",
        "Note that the data pre-processing routine below normalizes each feature. You will learn more about Feature transformations in Week 5.\n",
        "\n",
        "In what follows, we use Cross-Validation to select the best hyperparamters for gradient descent on the ridge regression model. Using the best hyperparameters, we will then make predictions on a reserved test set. You will also compare the results when using the gradient descent based implementation vs the analytic (closed form) solution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT EDIT BELOW.\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Pre-Processing the data and returning the train and test sets.\n",
        "\n",
        "# load the dataset and do some data exploration\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "\n",
        "raw_data = np.concatenate((data, target[:, None]), axis=1)\n",
        "xvars = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]\n",
        "yvars = [\"TARGET_VAL\"]\n",
        "\n",
        "data = pd.DataFrame(raw_data, columns=xvars+yvars)\n",
        "\n",
        "# Get the train and test splits to be used later.\n",
        "X_train, y_train, X_test, y_test = get_data_splits_with_transforms(data, xvars, yvars)"
      ],
      "metadata": {
        "id": "3Zf7gYm-E_gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Z4plUevNrU"
      },
      "source": [
        "**CODE REQUIRED HERE** Before we start using the Boston Housing dataset, let's implement the `ridge_gd` function. Given an input `X_train`, `y_train`, `lam`, `theta`, `step_size_fn`, and `num_steps`, run gradient descent on `X_train` and `y_train` starting from `theta = (dx1) vector of zeros`. Return the value of $\\theta$ after running `num_steps` iterations of gradient descent. \n",
        "\n",
        "```\n",
        "inputs: \n",
        "  X_train: a dxn numpy array \n",
        "  y_train: a 1xn numpy array \n",
        "  lam: lambda \n",
        "  step_size_fn: a function that takes in i, the current training iteration, and returns the step size for iteration i \n",
        "  num_steps: number of iterations\n",
        "\n",
        "outputs: \n",
        "  theta: value of theta after num_steps iterations of gradient descent \n",
        "```\n",
        "**Hint**: Your implementations of `objective_func` and `objective_func_grad` are very useful here! \\\n",
        "**Hint**: You can also use your `gd` function \\\n",
        "**Hint**: Previously, you've minimized f as a function of x. Now, X and y are constant. What variable are you minimizing over now? \\"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ridge_gd(X_train, y_train, lam, step_size_fn, num_steps=2000): \n",
        "    # TODO \n",
        "    # hint: start from theta = (dx1) vector of zeros\n",
        "    pass"
      ],
      "metadata": {
        "id": "nf6OI1PCQoxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TestRidgeGD(ridge_gd)"
      ],
      "metadata": {
        "id": "7r3byrS1LRgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20b1e12-2251-493e-cb66-e19fd112f125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test case passed!\n",
            "----------------------------\n",
            "\n",
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcyFKWP233ir"
      },
      "source": [
        "**CODE REQUIRED HERE** In `cross_validate_gd`, run 5-fold cross-validation on the X and y dataset. Use gradient descent to train a linear model for the `X`, `y` data. We've provided a for loop that iterates over each split. In this code: \n",
        "\n",
        "```\n",
        "  X_train_split, y_train_split: data to use for training. This is a (d x n) numpy array, where n=the number of datapoints in k-1 folds \n",
        "  X_val_split, y_val_split: data to use for evaluating the model. This is a (d x n) numpy array, where n=the number of datapoints in 1 fold\n",
        "```\n",
        "\n",
        "**Hint**: Use `ridge_gd` here. \\\n",
        "**Hint**: Take a look at the solutions for last week's cross_validate code if you get stuck"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validate_gd(X, y, lam, step_size_fn):\n",
        "    \"\"\"\n",
        "    Returns k-fold cross-validation loss. On each of the k folds, \n",
        "    train a linear regression model using gradient descent. Return \n",
        "    the average loss across the k folds. \n",
        "    \"\"\"\n",
        "    total_loss = 0\n",
        "    kf = KFold(n_splits=5)\n",
        "    for train_index, test_index in kf.split(X, y=y):\n",
        "        X_train_split, y_train_split = X[train_index].T, y[train_index].T\n",
        "        # TODO - train model on X_train_split, y_train_split using gradient descent\n",
        "        # hint - use variables step_size_fn and lam\n",
        "        X_val_split, y_val_split = X[test_index].T, y[test_index].T\n",
        "        # TODO - evaluate model on X_val_split, y_val_split, add loss to total_loss\n",
        "    return total_loss / kf.n_splits"
      ],
      "metadata": {
        "id": "jx4A_489Qk2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMkgs4rfXWMz"
      },
      "source": [
        "Now it's time to run grid search! We are interested in running grid search over $\\lambda \\in \\{{1e-4, 1e-3, \\cdots, 1e-1\\}}$ and $\\eta \\in \\{{1e-6, 1e-5, \\cdots, 1e-2\\}}$. \n",
        "\n",
        "These two cells are ready to run if you've correctly implemented `cross_validate_gd`. Use the outputs of these cells to answer the rest of problem 5.2. \n",
        "\n",
        "We've also already implemented `cross_validate_analytic` for you. This function returns the cross-validation loss for linear regression models trained with the analytic solution for the squared loss equation. \n",
        "\n",
        "**Note: The next two cells print the cross-validation loss, not the testing set loss! Run the last cell in this notebook for the testing set loss.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nttJDNnpc18o"
      },
      "source": [
        "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
        "lams = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "\n",
        "# This code runs grid search over the training parameters in `learning_rates` and `lams`\n",
        "for rate in learning_rates:\n",
        "    for lam in lams:\n",
        "        learning_rate_fn = lambda i : rate # learning rate = `rate` throughout training\n",
        "        cross_validation_loss = cross_validate_gd(X_train, y_train, lam, learning_rate_fn)\n",
        "        print(f\"Loss on dataset with lambda={lam}, rate={rate} : cross_validation_loss {cross_validation_loss:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDvZz6A4NDBF"
      },
      "source": [
        "lams = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "\n",
        "# This code runs grid search over the training parameters in `lams`\n",
        "for lam in lams:\n",
        "    cross_validation_loss = cross_validate_analytic(X_train, y_train, lam).item()\n",
        "    print(f\"Loss on dataset with lambda={lam}: cross_validation_loss {cross_validation_loss:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RKWyzPRkQXl"
      },
      "source": [
        "We will now use the best params found above to build a model on the entire training set (X_train, y_train), get the $\\theta$ values and use them to make predictions for the test set (X_test) and evaluate the error using the *actual* values (y_test). We will compare this error for the gradient descent based implementation vs the analytic solution.\n",
        "\n",
        "\n",
        "**CODE REQUIRED HERE**:\n",
        "\n",
        "1. Update **best_lam_gd** and **best_rate_gd** using the best $\\lambda$ and $\\eta$ values you found using **cross_validate_gd**() above.\n",
        "\n",
        "2. Update **best_lam_analytic** using the best $\\lambda$ value found by using **cross_validate_analytic**() above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns the test set predictions and error for the GD based implementation.\n",
        "def get_gd_predictions_and_error(objective_func, objective_func_grad, gd_func, X_train, y_train, X_test, y_test, best_lam_gd, best_rate_gd):\n",
        "    num_steps = 5000\n",
        "    theta_gd = ridge_gd(X_train.T, y_train.T, best_lam_gd, lambda i: best_rate_gd, num_steps=num_steps)\n",
        "    gd_predictions = theta_gd.T @ X_test.T\n",
        "    gd_error = np.mean((gd_predictions - y_test)**2)\n",
        "    return gd_predictions, gd_error\n",
        "\n",
        "# Returns the test set predictions and error using the Analytic expression.\n",
        "def get_analytic_predictions_and_error(X_train, y_train, X_test, y_test, best_lam):\n",
        "    theta_analytic = ridge_analytic(X_train.T, y_train.T, best_lam)\n",
        "    analytic_predictions = theta_analytic.T @ X_test.T\n",
        "    analytic_error = np.mean((analytic_predictions - y_test)**2)\n",
        "    return analytic_predictions, analytic_error"
      ],
      "metadata": {
        "id": "wBfbRlxRMqUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhJFMnVUjm-Z"
      },
      "source": [
        "#### Using the functions above along with the best performing hyperparams to\n",
        "### determine the test set errors. Please specify the best lambda and learning \n",
        "### rates for the GD and Analytic cases that you found above.\n",
        "\n",
        "# GD\n",
        "best_lam_gd = None   ### TODO: to be specified\n",
        "best_rate_gd = None  ### TODO: to be specified\n",
        "\n",
        "# get_gd_predictions() function is defined in the hw03 code you imported at \n",
        "# the very top. Check the code out if you are curious about the implementation.\n",
        "gd_predictions, gd_error = get_gd_predictions_and_error(\n",
        "    objective_func, objective_func_grad, gd, X_train, y_train, X_test, y_test, best_lam_gd, best_rate_gd)\n",
        "\n",
        "# Analytic\n",
        "best_lam_analytic = None  ### TODO: to be specified\n",
        "\n",
        "# get_analytic_predictions_and_error() function is defined in the hw03 code \n",
        "# you imported at the very top. Check the code out if you are curious about the \n",
        "# implementation.\n",
        "analytic_predictions, analytic_error = get_analytic_predictions_and_error(\n",
        "    X_train, y_train, X_test, y_test, best_lam_analytic)\n",
        "\n",
        "\n",
        "print(f\"Test loss for GD based implementation={gd_error:0.3f}\")\n",
        "print(f\"Test loss for Analytic (closed form) implementation={analytic_error:0.3f}\")\n",
        "\n",
        "\n",
        "#### (Optional) Compare the results by viewing the scatter plots for predictions.\n",
        "plt.scatter(y_test, gd_predictions, color='red', label='GD')\n",
        "plt.scatter(y_test, analytic_predictions, color='blue', label='Analytic')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predictions')\n",
        "plt.title('Predictions vs Actual Scatter Plot')\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([-3, 3])\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}